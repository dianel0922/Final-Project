{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM(n-gram).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KRFlk4pD35b",
        "outputId": "78fab90d-32c3-4b11-f45d-3115b4b25e9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CusJm-IUDyL8"
      },
      "outputs": [],
      "source": [
        "# basic libraries\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from zipfile import ZipFile\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# n-gram\n",
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List # somehow python 3.9 requires\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "# from sklearn.metrics import precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# keras lstm model\n",
        "from keras.layers import Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras.backend as K\n",
        "\n",
        "\"\"\"\n",
        "Config\n",
        "跑之前要改config = writing_patterns/Quantity/Both\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "writing_patterns = ['title_special_chars', 'text_special_chars',\n",
        "       'title_dets', 'text_dets', 'title_capitals', 'text_capitals',\n",
        "       'title_short_sents', 'text_short_sents', 'title_long_sents',\n",
        "       'text_long_sents']\n",
        "\n",
        "Quantity = ['title_words', 'text_words',\n",
        "       'title_sents', 'text_sents', 'title_words_per_sent',\n",
        "       'text_words_per_sent', 'title_verbs', 'text_verbs', 'title_adjs',\n",
        "       'text_adjs', 'title_advs', 'text_advs', 'title_rate_adjs_and_advs',\n",
        "       'text_rate_adjs_and_advs','title_fog', 'text_fog', 'title_smog', 'text_smog',\n",
        "       'title_ari', 'text_ari']\n",
        "\n",
        "Both = ['title_words', 'text_words',\n",
        "       'title_sents', 'text_sents', 'title_words_per_sent',\n",
        "       'text_words_per_sent', 'title_verbs', 'text_verbs', 'title_adjs',\n",
        "       'text_adjs', 'title_advs', 'text_advs', 'title_rate_adjs_and_advs',\n",
        "       'text_rate_adjs_and_advs', 'title_special_chars', 'text_special_chars',\n",
        "       'title_dets', 'text_dets', 'title_capitals', 'text_capitals',\n",
        "       'title_short_sents', 'text_short_sents', 'title_long_sents',\n",
        "       'text_long_sents', 'title_fog', 'text_fog', 'title_smog', 'text_smog',\n",
        "       'title_ari', 'text_ari']\n",
        "\n",
        "\n",
        "config = writing_patterns\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Evaluation functions\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def mcc_metric(y_true, y_pred):\n",
        "  y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
        "  y_pred_neg = 1 - y_pred_pos\n",
        "\n",
        "  y_pos = K.round(K.clip(y_true, 0, 1))\n",
        "  y_neg = 1 - y_pos\n",
        "\n",
        "  tp = K.sum(y_pos * y_pred_pos)\n",
        "  tn = K.sum(y_neg * y_pred_neg)\n",
        "\n",
        "  fp = K.sum(y_neg * y_pred_pos)\n",
        "  fn = K.sum(y_pos * y_pred_neg)\n",
        "\n",
        "  numerator = (tp * tn - fp * fn)\n",
        "  denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "\n",
        "  return numerator / (denominator + K.epsilon())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "get Dataset from Drive\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/gdrive/') # 此處需要登入google帳號\n",
        "!unzip -u \"/content/gdrive/MyDrive/人工智慧概論23組/dataset (with valid).zip\" -d \"/content/\"\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "valid = pd.read_csv('valid.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VESkQB87WAUl",
        "outputId": "e1e03a83-1595-4d66-eb3a-642327bfc8c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "Archive:  /content/gdrive/MyDrive/人工智慧概論23組/dataset (with valid).zip\n",
            "  inflating: /content/test.csv       \n",
            "  inflating: /content/train.csv      \n",
            "  inflating: /content/valid.csv      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Ngram:\n",
        "    def __init__(self, config, n=2):\n",
        "        self.tokenizer = ToktokTokenizer()\n",
        "        self.n = n\n",
        "        self.model = None\n",
        "        self.config = config\n",
        "        self.counts = None # the counts for corpus\n",
        "\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        return self.tokenizer.tokenize(sentence)\n",
        "\n",
        "    #turning words that appeared less than 10/1 times as [UNK], num=10 or 2\n",
        "    def turnToUnk(self, corpus, num):\n",
        "        corpus_list = [j for i in corpus for j in i]\n",
        "        counts = Counter(corpus_list)\n",
        "        pdCounts = pd.DataFrame(counts.values(),index = list(counts))\n",
        "        lessWords = list(pdCounts[pdCounts[0]<num].index)\n",
        "        pd_corpus = pd.DataFrame(corpus_list)\n",
        "        pd_corpus[pd_corpus[0].isin(lessWords)] = '[UNK]'\n",
        "        pd_index = (pd_corpus[pd_corpus[0].str.contains('[CLS]',na=False)])\n",
        "        length = len(pd_index.index)\n",
        "        corpus = []\n",
        "        for i,f in enumerate(pd_index.index):\n",
        "            if(i == length-1):\n",
        "                break\n",
        "            ff = pd_index.index[i+1]\n",
        "            corpus.append(list(pd_corpus[f:ff][0]))\n",
        "        return corpus\n",
        "\n",
        "    def get_ngram(self, corpus_tokenize: List[List[str]]):\n",
        "        # begin your code (Part 1)\n",
        "        \n",
        "        bigram_tokens = [(x, i[j + 1]) for i in corpus_tokenize \n",
        "            for j, x in enumerate(i) if j < len(i) - 1]\n",
        "        bigrams_freq = Counter(bigram_tokens)\n",
        "\n",
        "        prob = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        counts = defaultdict(lambda: defaultdict(lambda :0))\n",
        "        for i in bigrams_freq:\n",
        "            counts[i[0]][i[1]] = bigrams_freq[i]\n",
        "        for i1 in counts:\n",
        "            total_count = float(sum(counts[i1].values()))\n",
        "            for i2 in counts[i1]:\n",
        "                prob[i1][i2] = counts[i1][i2]/ total_count\n",
        "\n",
        "        return prob, counts\n",
        "    \n",
        "    def train(self, df):\n",
        "        '''\n",
        "        Train n-gram model.\n",
        "        '''\n",
        "        corpus = [['[CLS]'] + self.tokenize(document) for document in df['text']] + [['[CLS]'] + self.tokenize(document) for document in df['title']] \n",
        "        # [CLS] represents start of sequence\n",
        "        \n",
        "        self.model, self.counts = self.get_ngram(corpus)\n",
        "                \n"
      ],
      "metadata": {
        "id": "sGLTYVkfWqJM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 主程式\n",
        "biGram = Ngram(2)\n",
        "biGram.train(train)"
      ],
      "metadata": {
        "id": "n_b74YYRW7-_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pick the first n features\n",
        "pass to a DataFrame and sort by their frequency\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "feature_num = 800\n",
        "feature_counter = []\n",
        "for i in biGram.counts:\n",
        "    for r in biGram.counts[i]:\n",
        "        feature_counter.append((i+\" \"+r,biGram.counts[i][r]))\n",
        "\n",
        "df = pd.DataFrame(feature_counter)\n",
        "df = df.sort_values(by=[1],ascending=False)\n",
        "feature = list(df.iloc[0:feature_num,0])"
      ],
      "metadata": {
        "id": "eToxm0NKWzGx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_train = []\n",
        "x_test = []\n",
        "x_valid = []\n",
        "\n",
        "\n",
        "for i,f in enumerate(train['text']):\n",
        "    corpus = [['[CLS]'] + biGram.tokenize(f)] + [['[CLS]'] + biGram.tokenize(train['title'][i])]\n",
        "\n",
        "    bigram_tokens = [(x+\" \"+i[j + 1]) for i in corpus \n",
        "                for j, x in enumerate(i) if j < len(i) - 1]\n",
        "    count = [0]* feature_num\n",
        "    for f in bigram_tokens:\n",
        "        if(f in feature):\n",
        "            count[feature.index(f)]+=1\n",
        "    x_train.append(count)\n",
        "\n",
        "\n",
        "\n",
        "for i,f in enumerate(test['text']):\n",
        "    corpus = [['[CLS]'] + biGram.tokenize(f)] + [['[CLS]'] + biGram.tokenize(test['title'][i])]\n",
        "    \n",
        "    bigram_tokens = [(x+\" \"+i[j + 1]) for i in corpus \n",
        "                for j, x in enumerate(i) if j < len(i) - 1]\n",
        "    count = [0]*feature_num\n",
        "    for f in bigram_tokens:\n",
        "        if(f in feature):\n",
        "            count[feature.index(f)]+=1\n",
        "    x_test.append(count)\n",
        "\n",
        "    \n",
        "for i,f in enumerate(valid['text']):\n",
        "    corpus = [['[CLS]'] + biGram.tokenize(f)] + [['[CLS]'] + biGram.tokenize(valid['title'][i])]\n",
        "    \n",
        "    bigram_tokens = [(x+\" \"+i[j + 1]) for i in corpus \n",
        "                for j, x in enumerate(i) if j < len(i) - 1]\n",
        "    count = [0]*feature_num\n",
        "    for f in bigram_tokens:\n",
        "        if(f in feature):\n",
        "            count[feature.index(f)]+=1\n",
        "    x_valid.append(count)\n"
      ],
      "metadata": {
        "id": "GtHJnVQ8wxGi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "x_valid = np.array(x_valid)\n",
        "\n",
        "\n",
        "y_train = train['label']\n",
        "y_val = valid['label']\n",
        "y_test = test['label']"
      ],
      "metadata": {
        "id": "vx-C50mVaC0h"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "從這邊開始是LSTM"
      ],
      "metadata": {
        "id": "iWQKR_jV4X6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_out = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(800, output_dim=256))\n",
        "model.add(LSTM(lstm_out))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, name='out_layer'))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam',\\\n",
        "                metrics = ['accuracy', recall_m, precision_m, mcc_metric])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYAGsh2fWIZb",
        "outputId": "477928d5-a0ef-405d-c08f-a4342ddeb58e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 256)         204800    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 64)                82176     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               16640     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 256)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " out_layer (Dense)           (None, 1)                 257       \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 303,873\n",
            "Trainable params: 303,873\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_valid, y_val), epochs=20, batch_size=128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCcrK6e2Y4Ji",
        "outputId": "9a8fb3d9-583e-4e49-9dbd-4e437569b01c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "310/310 [==============================] - 23s 50ms/step - loss: 0.6936 - accuracy: 0.4999 - recall_m: 0.4514 - precision_m: 0.5006 - mcc_metric: 0.0015 - val_loss: 0.6931 - val_accuracy: 0.4999 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_mcc_metric: 0.0000e+00\n",
            "Epoch 2/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6938 - accuracy: 0.5030 - recall_m: 0.5531 - precision_m: 0.4680 - mcc_metric: 0.0070 - val_loss: 0.6936 - val_accuracy: 0.4999 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_mcc_metric: 0.0000e+00\n",
            "Epoch 3/20\n",
            "310/310 [==============================] - 15s 48ms/step - loss: 0.6926 - accuracy: 0.5113 - recall_m: 0.5785 - precision_m: 0.5070 - mcc_metric: 0.0295 - val_loss: 0.6943 - val_accuracy: 0.4999 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_mcc_metric: 0.0000e+00\n",
            "Epoch 4/20\n",
            "310/310 [==============================] - 15s 50ms/step - loss: 0.6884 - accuracy: 0.5320 - recall_m: 0.5714 - precision_m: 0.5319 - mcc_metric: 0.0677 - val_loss: 0.6746 - val_accuracy: 0.5843 - val_recall_m: 0.5634 - val_precision_m: 0.5886 - val_mcc_metric: 0.1685\n",
            "Epoch 5/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6694 - accuracy: 0.5886 - recall_m: 0.7581 - precision_m: 0.5674 - mcc_metric: 0.1923 - val_loss: 0.6669 - val_accuracy: 0.5783 - val_recall_m: 0.3756 - val_precision_m: 0.6326 - val_mcc_metric: 0.1696\n",
            "Epoch 6/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6587 - accuracy: 0.5905 - recall_m: 0.6694 - precision_m: 0.5854 - mcc_metric: 0.1939 - val_loss: 0.6521 - val_accuracy: 0.5968 - val_recall_m: 0.8177 - val_precision_m: 0.5680 - val_mcc_metric: 0.2155\n",
            "Epoch 7/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6514 - accuracy: 0.5889 - recall_m: 0.6919 - precision_m: 0.5800 - mcc_metric: 0.1935 - val_loss: 0.6541 - val_accuracy: 0.5906 - val_recall_m: 0.8420 - val_precision_m: 0.5617 - val_mcc_metric: 0.2108\n",
            "Epoch 8/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6488 - accuracy: 0.5904 - recall_m: 0.6879 - precision_m: 0.5830 - mcc_metric: 0.1931 - val_loss: 0.6507 - val_accuracy: 0.6021 - val_recall_m: 0.6844 - val_precision_m: 0.5887 - val_mcc_metric: 0.2067\n",
            "Epoch 9/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6473 - accuracy: 0.5977 - recall_m: 0.6936 - precision_m: 0.5896 - mcc_metric: 0.2080 - val_loss: 0.6510 - val_accuracy: 0.5892 - val_recall_m: 0.8527 - val_precision_m: 0.5586 - val_mcc_metric: 0.2074\n",
            "Epoch 10/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6450 - accuracy: 0.5993 - recall_m: 0.7385 - precision_m: 0.5809 - mcc_metric: 0.2139 - val_loss: 0.6498 - val_accuracy: 0.5949 - val_recall_m: 0.8409 - val_precision_m: 0.5641 - val_mcc_metric: 0.2155\n",
            "Epoch 11/20\n",
            "310/310 [==============================] - 15s 50ms/step - loss: 0.6442 - accuracy: 0.6024 - recall_m: 0.6936 - precision_m: 0.5912 - mcc_metric: 0.2147 - val_loss: 0.6499 - val_accuracy: 0.6051 - val_recall_m: 0.7145 - val_precision_m: 0.5884 - val_mcc_metric: 0.2178\n",
            "Epoch 12/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6437 - accuracy: 0.6057 - recall_m: 0.7286 - precision_m: 0.5877 - mcc_metric: 0.2228 - val_loss: 0.6452 - val_accuracy: 0.6072 - val_recall_m: 0.6603 - val_precision_m: 0.5984 - val_mcc_metric: 0.2158\n",
            "Epoch 13/20\n",
            "310/310 [==============================] - 16s 51ms/step - loss: 0.6383 - accuracy: 0.6127 - recall_m: 0.7094 - precision_m: 0.5973 - mcc_metric: 0.2352 - val_loss: 0.6378 - val_accuracy: 0.6101 - val_recall_m: 0.8440 - val_precision_m: 0.5753 - val_mcc_metric: 0.2478\n",
            "Epoch 14/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6284 - accuracy: 0.6221 - recall_m: 0.7628 - precision_m: 0.5974 - mcc_metric: 0.2593 - val_loss: 0.6263 - val_accuracy: 0.6337 - val_recall_m: 0.7471 - val_precision_m: 0.6096 - val_mcc_metric: 0.2736\n",
            "Epoch 15/20\n",
            "310/310 [==============================] - 15s 48ms/step - loss: 0.6109 - accuracy: 0.6471 - recall_m: 0.7727 - precision_m: 0.6201 - mcc_metric: 0.3082 - val_loss: 0.5865 - val_accuracy: 0.6743 - val_recall_m: 0.8516 - val_precision_m: 0.6293 - val_mcc_metric: 0.3731\n",
            "Epoch 16/20\n",
            "310/310 [==============================] - 16s 52ms/step - loss: 0.5985 - accuracy: 0.6711 - recall_m: 0.6905 - precision_m: 0.6715 - mcc_metric: 0.3519 - val_loss: 0.6525 - val_accuracy: 0.6042 - val_recall_m: 0.5521 - val_precision_m: 0.6182 - val_mcc_metric: 0.2114\n",
            "Epoch 17/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6436 - accuracy: 0.6125 - recall_m: 0.6227 - precision_m: 0.6135 - mcc_metric: 0.2288 - val_loss: 0.6423 - val_accuracy: 0.6078 - val_recall_m: 0.5094 - val_precision_m: 0.6353 - val_mcc_metric: 0.2201\n",
            "Epoch 18/20\n",
            "310/310 [==============================] - 15s 49ms/step - loss: 0.6362 - accuracy: 0.6173 - recall_m: 0.6780 - precision_m: 0.6070 - mcc_metric: 0.2390 - val_loss: 0.6365 - val_accuracy: 0.6191 - val_recall_m: 0.6715 - val_precision_m: 0.6088 - val_mcc_metric: 0.2410\n",
            "Epoch 19/20\n",
            "310/310 [==============================] - 16s 51ms/step - loss: 0.6320 - accuracy: 0.6262 - recall_m: 0.6733 - precision_m: 0.6174 - mcc_metric: 0.2555 - val_loss: 0.6323 - val_accuracy: 0.6192 - val_recall_m: 0.7029 - val_precision_m: 0.6036 - val_mcc_metric: 0.2447\n",
            "Epoch 20/20\n",
            "310/310 [==============================] - 16s 51ms/step - loss: 0.6267 - accuracy: 0.6337 - recall_m: 0.6735 - precision_m: 0.6257 - mcc_metric: 0.2700 - val_loss: 0.6246 - val_accuracy: 0.6367 - val_recall_m: 0.6849 - val_precision_m: 0.6262 - val_mcc_metric: 0.2767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0t8VDRdZSpb",
        "outputId": "1af83118-810a-4df1-b64c-d7df72b8ee56"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "355/355 [==============================] - 6s 16ms/step - loss: 0.6146 - accuracy: 0.6437 - recall_m: 0.6932 - precision_m: 0.6293 - mcc_metric: 0.2893\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6145535707473755,\n",
              " 0.6437461376190186,\n",
              " 0.693248450756073,\n",
              " 0.6292709708213806,\n",
              " 0.28930288553237915]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DZyyGqMk6hPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}