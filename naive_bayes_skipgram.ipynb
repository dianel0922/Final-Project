{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "CusJm-IUDyL8"
      },
      "outputs": [],
      "source": [
        "# basic libraries\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from zipfile import ZipFile\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# n-gram\n",
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List # somehow python 3.9 requires\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "# from sklearn.metrics import precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Config\n",
        "跑之前要改config = writing_patterns/Quantity/Both\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "writing_patterns = ['title_special_chars', 'text_special_chars',\n",
        "       'title_dets', 'text_dets', 'title_capitals', 'text_capitals',\n",
        "       'title_short_sents', 'text_short_sents', 'title_long_sents',\n",
        "       'text_long_sents']\n",
        "\n",
        "Quantity = ['title_words', 'text_words',\n",
        "       'title_sents', 'text_sents', 'title_words_per_sent',\n",
        "       'text_words_per_sent', 'title_verbs', 'text_verbs', 'title_adjs',\n",
        "       'text_adjs', 'title_advs', 'text_advs', 'title_rate_adjs_and_advs',\n",
        "       'text_rate_adjs_and_advs','title_fog', 'text_fog', 'title_smog', 'text_smog',\n",
        "       'title_ari', 'text_ari']\n",
        "\n",
        "Both = ['title_words', 'text_words',\n",
        "       'title_sents', 'text_sents', 'title_words_per_sent',\n",
        "       'text_words_per_sent', 'title_verbs', 'text_verbs', 'title_adjs',\n",
        "       'text_adjs', 'title_advs', 'text_advs', 'title_rate_adjs_and_advs',\n",
        "       'text_rate_adjs_and_advs', 'title_special_chars', 'text_special_chars',\n",
        "       'title_dets', 'text_dets', 'title_capitals', 'text_capitals',\n",
        "       'title_short_sents', 'text_short_sents', 'title_long_sents',\n",
        "       'text_long_sents', 'title_fog', 'text_fog', 'title_smog', 'text_smog',\n",
        "       'title_ari', 'text_ari']\n",
        "\n",
        "\n",
        "config = Both\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VESkQB87WAUl",
        "outputId": "e1e03a83-1595-4d66-eb3a-642327bfc8c3"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/Users/kiyas/Downloads/dataset (with valid)/train.csv')\n",
        "test = pd.read_csv('/Users/kiyas/Downloads/dataset (with valid)/test.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## skip-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#%% train skip-gram model\n",
        "tokenizer = ToktokTokenizer()\n",
        "corpus = [tokenizer.tokenize(document) for document in train['title']]\n",
        "model = gensim.models.Word2Vec(corpus, min_count = 5, window = 5, sg = 1) # sg -> skip gram = true\n",
        "model.train(corpus, total_examples=len(corpus), epochs = 5)\n",
        "\n",
        "vector = model.wv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "words = model.wv.index_to_key\n",
        "wvs = model.wv[words]\n",
        "dic = {}\n",
        "for i,f in enumerate(words):\n",
        "    dic[f] = wvs[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = []\n",
        "\n",
        "for i,f in enumerate(train['text']):\n",
        "    one = np.zeros((500, 1))\n",
        "    corpus = [['[CLS]'] + tokenizer.tokenize(f)] + [['[CLS]'] + tokenizer.tokenize(train['title'][i])]\n",
        "        \n",
        "    if len(corpus[0]) < 500:\n",
        "        for i in range(500-len(corpus[0])):\n",
        "            corpus[0].append('0')\n",
        "\n",
        "    one=[]\n",
        "    for ii,ff in enumerate(corpus[0][0:500]):\n",
        "        try:\n",
        "            one.append(dic[ff].mean())\n",
        "        except:\n",
        "            one.append(0)\n",
        "            \n",
        "    # print(one)\n",
        "    x_train.append(one)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "x_test = []\n",
        "for i,f in enumerate(test['text']):\n",
        "    corpus = [['[CLS]'] + tokenizer.tokenize(f)] + [['[CLS]'] + tokenizer.tokenize(test['title'][i])]\n",
        "        \n",
        "    if len(corpus[0]) < 500:\n",
        "        for i in range(500-len(corpus[0])):\n",
        "            corpus[0].append('0')\n",
        "\n",
        "    one=[]\n",
        "    for ii,ff in enumerate(corpus[0][0:500]):\n",
        "        try:\n",
        "            one.append(dic[ff].mean())\n",
        "        except:\n",
        "            one.append(0)\n",
        "            \n",
        "    # print(one)\n",
        "    x_test.append(one)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "vx-C50mVaC0h"
      },
      "outputs": [],
      "source": [
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 後面有feature的\n",
        "x_train = np.concatenate([x_train, train[config]], axis=1)\n",
        "x_test = np.concatenate([x_test, test[config]], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train = train['label']\n",
        "y_test = test['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "DZyyGqMk6hPk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.5833688906891188, Precision: 0.6198457918963036, Recall: 0.6003355766401883\n",
            "accuracy: 0.6003177685585666\n",
            "mcc: 0.2193152674832797\n"
          ]
        }
      ],
      "source": [
        "nb_model = GaussianNB()\n",
        "nb_model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_predicted = nb_model.predict(x_test)\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_predicted, average='macro', zero_division=1)\n",
        "print(f\"F1 score: {f1}, Precision: {precision}, Recall: {recall}\")\n",
        "\n",
        "acc = accuracy_score(y_test, y_predicted)\n",
        "print(f\"accuracy: {acc}\")\n",
        "\n",
        "mcc = matthews_corrcoef(y_test, y_predicted)\n",
        "print(f\"mcc: {mcc}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LSTM(n-gram).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "2f7ea0c6fc7121228ffaf76bc7d7e000b3b3e8c394302f55120707a69de8cdcb"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
